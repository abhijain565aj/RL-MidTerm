{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output'\n",
    "# Create directories to save model output\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "with open(\"scores.txt\", \"w\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I tried the game breakdown - v4 but that didn't worked out. It gave me tough times in installation and debugging because the atari version of gym don't work with pythons >= 3.10. So, I tried to create conda environment and the different installations all messed up as google colab didn't worked then so I switched back to the Cartpole - v1 game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\") # Create the environment for the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.0005\n",
    "\n",
    "batch_size = 32\n",
    "n_episodes = 50\n",
    "deque_memory_size = 2000\n",
    "output_dir = 'model_output/cartpole/'\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "gamma = 0.999 #discount factor\n",
    "\n",
    "#environment variables\n",
    "state_size = env.observation_space.shape[0]  # 4 = cart position, cart velocity, pole angle, pole velocity at the tip\n",
    "action_size = env.action_space.n #two actions left and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided Markdown snippet outlines the process of defining a Deep Q-Network (DQN) model. It describes the creation of a sequential model using TensorFlow's Keras API, detailing the addition of two dense layers with ReLU activation functions, followed by an output layer with a linear activation function to predict Q-values for each possible action. \n",
    "The model is compiled with the Mean Squared Error (MSE) loss function and the Adam optimizer, highlighting the key steps in constructing and preparing the DQN model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_size = input_dimention = 4 and action_size = output_dimention = 2\n",
    "# Define the DQN model\n",
    "def build_model(): \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    # model.add(layers.Dense(16, input_dim=32, activation='relu'))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.model = build_model() #primary model\n",
    "        self.target_model = build_model() #target model\n",
    "        self.update_target_model() #copy the weights from model to target model\n",
    "        \n",
    "    def update_target_model(self): #update the target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done): #store the state, action, reward, next_state and done in memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state): #choose the action based on epsilon greedy policy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) #predict the action values\n",
    "        return np.argmax(act_values[0]) #return the action with maximum value\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if(len(self.memory) < batch_size): #if the memory is less than batch size then return\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state =np.array(state).reshape([1, state_size])\n",
    "            next_state = np.array(next_state).reshape([1, state_size])\n",
    "            # Predict Q-values for current state using the primary model\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                # If the episode is done, set the target for the action to the reward\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # Predict Q-values for the next state using the target model\n",
    "                t = self.target_model.predict(next_state)\n",
    "                # Set the target for the action to reward plus the discounted max Q-value of the next state\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            # Fit the model with the updated target values\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the agent\n",
    "agent = DQNAgent()\n",
    "done = False\n",
    "scores = [] #store the scores\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    if(type(state) is tuple):\n",
    "        state = np.array(state[0]).reshape([1, state_size])\n",
    "    print(state)\n",
    "    done = False\n",
    "    tReward = 0\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        step_action = env.step(action)\n",
    "        # print(step_action)\n",
    "\n",
    "        next_state = np.array(step_action[0]).reshape([1, state_size])\n",
    "        reward = step_action[1]\n",
    "        done = step_action[2]\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)     \n",
    "        state = next_state\n",
    "        \n",
    "        tReward += reward\n",
    "        agent.replay(batch_size)\n",
    "    \n",
    "    agent.update_target_model()\n",
    "    scores.append(tReward)\n",
    "    print(f\"Episode: {e}/{n_episodes}, Score: {tReward}, Epsilon: {agent.epsilon:.2}\")\n",
    "    # appending the score in a file\n",
    "    with open('scores.txt', 'a') as file:\n",
    "        file.write(f\"Episode: {e}/{n_episodes}, Score: {tReward}, Epsilon: {agent.epsilon:.2}\\n\")\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "agent.save(output_dir + 'model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_scores\n",
    "print(scores)\n",
    "#plotting the tRewards\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Total Rewards in each episode')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
